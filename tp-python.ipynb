{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achraf JDAY 4/12/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install click\n",
    "!python3 -m pip install requests\n",
    "!python3 -m pip install csv\n",
    "!python3 -m pip install bs4\n",
    "!python3 -m pip install datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import datetime\n",
    "import click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.links = []\n",
    "        self.links_with_error_404 = []\n",
    "        self.links_with_authentication_401 = []\n",
    "        self.links_scanned = []\n",
    "\n",
    "    def isValidUrl(self,sous_url:str)->bool:\n",
    "\n",
    "        if sous_url not in self.links and sous_url != \"#\" and sous_url.startswith(self.url):\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "\n",
    "\n",
    "    def getPage(self) -> requests.models.Response:\n",
    "\n",
    "        page = requests.get(self.url)\n",
    "        return page\n",
    "\n",
    "\n",
    "    def retrieveLinks(self):\n",
    "\n",
    "        bs = BeautifulSoup(self.getPage().content, \"html.parser\")\n",
    "        web_links = bs.select('a')\n",
    "        actual_web_links = [web_link['href'] for web_link in web_links]\n",
    "        for elt in actual_web_links :\n",
    "            if self.isValidUrl(elt) :\n",
    "                self.links.append(elt)\n",
    "        return\n",
    "\n",
    "    def exportInCsvFile(self, liste_a_exporter:list):\n",
    "\n",
    "        print(\"Création du fichier CSV.\")\n",
    "        with open(\"links.csv\", 'w',newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Liste horodatée GMT du %s\" % datetime.datetime.utcnow()]) \n",
    "            for i in range(0, len(liste_a_exporter)):\n",
    "                writer.writerow([liste_a_exporter[i]]) \n",
    "\n",
    "        return\n",
    "\n",
    "    def filterLinks404(self):\n",
    "\n",
    "        for i in range(0,len(self.links)):\n",
    "            try:\n",
    "                response = requests.get(self.links[i])\n",
    "                if response.status_code == 404:\n",
    "                    self.links_with_error_404.append(self.links[i])\n",
    "            except :\n",
    "                print(\"Exception Error\")\n",
    "        print(\"Vous avez %d lien(s) 404.\" %len(self.links_with_error_404))\n",
    "\n",
    "        return\n",
    "\n",
    "    def filterListAuthenticate401(self):\n",
    "\n",
    "        for i in range(0, len(self.links)):\n",
    "            try:\n",
    "                authi = requests.get(self.links[i], auth=(\"\",\"\"))\n",
    "                if authi.status_code == 401:\n",
    "                    self.links_with_authentication_401.append(self.links[i])\n",
    "            except:\n",
    "                print(\"Exception Error : lien %s\" %(self.links[i]))\n",
    "        print(\"Vous avez %d lien(s) avec authentification.\" %len(self.links_with_authentication_401))\n",
    "        return\n",
    "\n",
    "    def retrieveAbsoluteAllLinks(self):\n",
    "        self.retrieveLinks() \n",
    "        self.links_scanned.append(self.url) \n",
    "        indice_initial = 0\n",
    "        indice_final = len(self.links)-1\n",
    "        print(\"L'indice initial est %d, le final est %d\" %(indice_initial,indice_final))\n",
    "\n",
    "\n",
    "        while len(self.links) != len(self.links_scanned): \n",
    "            print(\"Nbr de liens : %d. Nbr de liens scannés : %d.\" %(len(self.links), len(self.links_scanned)))\n",
    "            for i in range(indice_initial,indice_final):\n",
    "                try:\n",
    "                    print(\"Scan du lien %d. Liens totaux = %d\" %(i,len(self.links)))\n",
    "                    page = requests.get(self.links[i]) \n",
    "                    bs = BeautifulSoup(page.content, \"html.parser\")\n",
    "                    web_links = bs.select('a')\n",
    "                    actual_web_links = [web_link['href'] for web_link in web_links]\n",
    "                    for elt in actual_web_links:\n",
    "                        if self.isValidUrl(elt):\n",
    "                            self.links.append(elt)  \n",
    "                    self.links_scanned.append(self.links[i]) \n",
    "                    indice = i+1\n",
    "                except:\n",
    "                    print(\"Le lien %s ne peut être lu.\" %self.links[i])\n",
    "                    self.links_scanned.append(self.links[i])\n",
    "                    indice = i+1\n",
    "            indice_initial = indice\n",
    "            indice_final = len(self.links)-1\n",
    "        print(\"Scan de %s terminé. %d liens trouvés au total. %d liens scannés.\" %(self.url, len(self.links), len(self.links_scanned)))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@click.command()\n",
    "@click.option('--error404', default=False, help='Error404 links.')\n",
    "@click.option('--url', help= \"Enter the crawlable URL\")\n",
    "@click.option('--auth', default=False, help = \"Authentication links.\")\n",
    "@click.option('--export',default=False, help = \"Export in a csv file the list of links.\")\n",
    "def crawl(url, error404,export,auth):\n",
    "\n",
    "    site_a_crowler = crawler(url)\n",
    "    site_a_crowler.retrieveAbsoluteAllLinks()\n",
    "    if error404 :\n",
    "        site_a_crowler.filterLinks404()\n",
    "        if export :\n",
    "            site_a_crowler.exportInCsvFile(site_a_crowler.links_with_error_404)\n",
    "        else:\n",
    "            click.echo(site_a_crowler.links_with_error_404)\n",
    "            print(site_a_crowler.links_with_error_404)\n",
    "    elif auth :\n",
    "        site_a_crowler.filterListAuthenticate401()\n",
    "        if export:\n",
    "            site_a_crowler.exportInCsvFile(site_a_crowler.links_with_authentication_401)\n",
    "        else:\n",
    "            print(site_a_crowler.links_with_authentication_401)\n",
    "    else:\n",
    "        if export:\n",
    "            site_a_crowler.exportInCsvFile(site_a_crowler.links_scanned)\n",
    "        else:\n",
    "            print(site_a_crowler.links_scanned)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl(url = 'www.aforp.fr' , error404 = True , export = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl(url='www.aforp.fr' , auth = True , export = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also export the code to a .py file to test it on the command line as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python3 Click2.py --url='www.aforp.fr' --error404=True --export=True\n",
    "    python3 Click2.py --url='www.aforp.fr' --auth=True --export=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8489ca4a4db7ac94d15062bd0f7dc3ac7df730b94a150c9dd60c306ab1682441"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
